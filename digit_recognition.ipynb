{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition\n",
    "\n",
    "Using the digits dataset from sklearn, different classifier models are tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, tree, svm, linear_model, metrics, preprocessing, pipeline, ensemble\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10,3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    ax.set_title('Training: %i' %label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the images\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "X = data\n",
    "y = digits.target\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.4, random_state=0)\n",
    "\n",
    "# cross validation folds\n",
    "cv = 10\n",
    "\n",
    "\n",
    "# test, show pictures\n",
    "def test_classifier(predicted, clf):\n",
    "    _, axes = plt.subplots(nrows=1, ncols=7, figsize=(10,3))\n",
    "    for ax, image, prediction in zip(axes, X_test, predicted):\n",
    "        ax.set_axis_off()\n",
    "        image = image.reshape(8,8)\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title(f'Prediction: {prediction}')\n",
    "\n",
    "    print(f\"Classifier score: {clf.score(X_test, y_test)}\")\n",
    "\n",
    "    print(f\"Classification report for classifier {clf}:\\n\"\n",
    "          f\"{metrics.classification_report(y_test, predicted)}\\n\")\n",
    "    \n",
    "# cross validation\n",
    "def cross_validate(X, y, clf, cv):\n",
    "    scores = cross_val_score(clf, X, y, cv=cv)\n",
    "    print(f\"Cross validation score, using {cv} folds\")\n",
    "    for i, score in enumerate(scores):\n",
    "        print(f\"Test {i}: {score}\")\n",
    "    print(f\"Mean score: {np.mean(scores)}, std: {np.std(scores)}\")\n",
    "    \n",
    "\n",
    "# use grid search to find optimal parameters\n",
    "def fit_grid_search_params(parameters, clf, cv=cv):\n",
    "    clf = GridSearchCV(clf, parameters, cv=cv)\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(f\"Optimal setting: {clf.best_params_}\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(\"Results on test set\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'n_neighbors': 1}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "           1       0.97      1.00      0.99        73\n",
      "           2       1.00      1.00      1.00        71\n",
      "           3       0.96      1.00      0.98        70\n",
      "           4       1.00      1.00      1.00        63\n",
      "           5       0.99      0.98      0.98        89\n",
      "           6       0.99      1.00      0.99        76\n",
      "           7       1.00      1.00      1.00        65\n",
      "           8       1.00      0.95      0.97        78\n",
      "           9       0.97      0.96      0.97        74\n",
      "\n",
      "    accuracy                           0.99       719\n",
      "   macro avg       0.99      0.99      0.99       719\n",
      "weighted avg       0.99      0.99      0.99       719\n",
      "\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9444802201582387\n",
      "Test 1: 1.0\n",
      "Test 2: 0.99\n",
      "Test 3: 0.9751587301587301\n",
      "Test 4: 0.968328173374613\n",
      "Test 5: 0.9839181286549709\n",
      "Test 6: 0.9947368421052631\n",
      "Test 7: 0.9894736842105264\n",
      "Test 8: 0.9844736842105263\n",
      "Test 9: 0.9738888888888889\n",
      "Mean score: 0.9804458351761756, std: 0.01516236449157621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# find optimal K\n",
    "k_values = {'n_neighbors': [1,2,3,4,5,6,7,8,9,10]}\n",
    "\n",
    "knn = fit_grid_search_params(k_values, KNeighborsClassifier())\n",
    "\n",
    "cross_validate(X, y, knn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'max_depth': 10}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.94        60\n",
      "           1       0.73      0.84      0.78        73\n",
      "           2       0.93      0.73      0.82        71\n",
      "           3       0.75      0.86      0.80        70\n",
      "           4       0.90      0.84      0.87        63\n",
      "           5       0.85      0.83      0.84        89\n",
      "           6       0.96      0.96      0.96        76\n",
      "           7       0.80      0.92      0.86        65\n",
      "           8       0.73      0.58      0.64        78\n",
      "           9       0.84      0.84      0.84        74\n",
      "\n",
      "    accuracy                           0.83       719\n",
      "   macro avg       0.84      0.84      0.83       719\n",
      "weighted avg       0.84      0.83      0.83       719\n",
      "\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.7722222222222223\n",
      "Test 1: 0.8666666666666667\n",
      "Test 2: 0.8555555555555555\n",
      "Test 3: 0.7944444444444444\n",
      "Test 4: 0.7833333333333333\n",
      "Test 5: 0.8833333333333333\n",
      "Test 6: 0.8555555555555555\n",
      "Test 7: 0.8268156424581006\n",
      "Test 8: 0.7988826815642458\n",
      "Test 9: 0.8044692737430168\n",
      "Mean score: 0.8241278708876474, std: 0.03682490361379064\n"
     ]
    }
   ],
   "source": [
    "# create decision tree classifier\n",
    "params = {'max_depth': [1,3,5,10,12,15,17,20]}\n",
    "dec_tree = fit_grid_search_params(params, tree.DecisionTreeClassifier())\n",
    "\n",
    "cross_validate(X, y, dec_tree, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9819193324061196\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9444444444444444\n",
      "Test 1: 0.9944444444444445\n",
      "Test 2: 0.9777777777777777\n",
      "Test 3: 0.9111111111111111\n",
      "Test 4: 0.9777777777777777\n",
      "Test 5: 0.9888888888888889\n",
      "Test 6: 0.9777777777777777\n",
      "Test 7: 0.9441340782122905\n",
      "Test 8: 0.9608938547486033\n",
      "Test 9: 0.9553072625698324\n",
      "Mean score: 0.9632557417752947, std: 0.02402037522167597\n",
      "\n",
      "Using grid search\n",
      "Optimal setting: {'svc__C': 2.5, 'svc__degree': 2, 'svc__kernel': 'poly'}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "           1       0.99      0.99      0.99        73\n",
      "           2       0.99      0.99      0.99        71\n",
      "           3       0.96      0.97      0.96        70\n",
      "           4       1.00      0.98      0.99        63\n",
      "           5       0.98      0.96      0.97        89\n",
      "           6       0.99      0.99      0.99        76\n",
      "           7       0.98      0.98      0.98        65\n",
      "           8       0.96      0.99      0.97        78\n",
      "           9       0.97      0.97      0.97        74\n",
      "\n",
      "    accuracy                           0.98       719\n",
      "   macro avg       0.98      0.98      0.98       719\n",
      "weighted avg       0.98      0.98      0.98       719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create support vector machine classifier\n",
    "\n",
    "clf = pipeline.make_pipeline(preprocessing.StandardScaler(), svm.SVC())\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"Score: {clf.score(X_test, y_test)}\")\n",
    "cross_validate(X, y, clf, cv)\n",
    "\n",
    "print()\n",
    "\n",
    "param_grid = [\n",
    "    {'svc__C': np.linspace(1, 3, 5), 'svc__gamma': np.linspace(0, 1, 4), 'svc__kernel': ['rbf']},\n",
    "    {'svc__C': np.linspace(1, 3, 5), 'svc__degree': [2, 3, 4, 5], 'svc__kernel': ['poly']},\n",
    "]\n",
    "\n",
    "print(\"Using grid search\")\n",
    "svc = fit_grid_search_params(param_grid, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        60\n",
      "           1       0.93      0.88      0.90        73\n",
      "           2       0.85      0.97      0.91        71\n",
      "           3       0.94      0.91      0.93        70\n",
      "           4       0.95      0.95      0.95        63\n",
      "           5       0.98      0.96      0.97        89\n",
      "           6       0.93      0.97      0.95        76\n",
      "           7       0.94      0.97      0.95        65\n",
      "           8       0.98      0.77      0.86        78\n",
      "           9       0.83      0.91      0.86        74\n",
      "\n",
      "    accuracy                           0.93       719\n",
      "   macro avg       0.93      0.93      0.93       719\n",
      "weighted avg       0.93      0.93      0.93       719\n",
      "\n",
      "\n",
      "Using cross validation\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9222222222222223\n",
      "Test 1: 0.9388888888888889\n",
      "Test 2: 0.9111111111111111\n",
      "Test 3: 0.8388888888888889\n",
      "Test 4: 0.9111111111111111\n",
      "Test 5: 0.8944444444444445\n",
      "Test 6: 0.9555555555555556\n",
      "Test 7: 0.9497206703910615\n",
      "Test 8: 0.8491620111731844\n",
      "Test 9: 0.88268156424581\n",
      "Mean score: 0.905378646803228, std: 0.03764600144282289\n"
     ]
    }
   ],
   "source": [
    "# create ridge classifier\n",
    "rc = linear_model.RidgeClassifier()\n",
    "rc = rc.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Classification report:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\")\n",
    "\n",
    "print(\"Using cross validation\")\n",
    "cross_validate(X, y, rc, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'alpha': 2.894736842105263}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.98        60\n",
      "           1       0.92      0.89      0.90        73\n",
      "           2       0.87      0.96      0.91        71\n",
      "           3       0.94      0.91      0.93        70\n",
      "           4       0.97      0.95      0.96        63\n",
      "           5       0.96      0.96      0.96        89\n",
      "           6       0.95      0.95      0.95        76\n",
      "           7       0.94      0.97      0.95        65\n",
      "           8       0.97      0.78      0.87        78\n",
      "           9       0.82      0.91      0.86        74\n",
      "\n",
      "    accuracy                           0.92       719\n",
      "   macro avg       0.93      0.93      0.93       719\n",
      "weighted avg       0.93      0.92      0.92       719\n",
      "\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9222222222222223\n",
      "Test 1: 0.9388888888888889\n",
      "Test 2: 0.9111111111111111\n",
      "Test 3: 0.8388888888888889\n",
      "Test 4: 0.9111111111111111\n",
      "Test 5: 0.8944444444444445\n",
      "Test 6: 0.9555555555555556\n",
      "Test 7: 0.9497206703910615\n",
      "Test 8: 0.8491620111731844\n",
      "Test 9: 0.888268156424581\n",
      "Mean score: 0.9059373060211051, std: 0.037345286944261875\n"
     ]
    }
   ],
   "source": [
    "# use grid search to tune alpha\n",
    "params = {'alpha': np.linspace(1, 4, 20)}\n",
    "rc = fit_grid_search_params(params, linear_model.RidgeClassifier())\n",
    "\n",
    "cross_validate(X, y, rc, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        60\n",
      "           1       0.93      0.88      0.90        73\n",
      "           2       0.85      0.97      0.91        71\n",
      "           3       0.94      0.91      0.93        70\n",
      "           4       0.95      0.95      0.95        63\n",
      "           5       0.98      0.96      0.97        89\n",
      "           6       0.93      0.97      0.95        76\n",
      "           7       0.94      0.97      0.95        65\n",
      "           8       0.98      0.77      0.86        78\n",
      "           9       0.83      0.91      0.86        74\n",
      "\n",
      "    accuracy                           0.93       719\n",
      "   macro avg       0.93      0.93      0.93       719\n",
      "weighted avg       0.93      0.93      0.93       719\n",
      "\n",
      "\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9333333333333333\n",
      "Test 1: 0.95\n",
      "Test 2: 0.9222222222222223\n",
      "Test 3: 0.85\n",
      "Test 4: 0.9111111111111111\n",
      "Test 5: 0.8944444444444445\n",
      "Test 6: 0.9611111111111111\n",
      "Test 7: 0.9441340782122905\n",
      "Test 8: 0.8435754189944135\n",
      "Test 9: 0.888268156424581\n",
      "Mean score: 0.9098199875853508, std: 0.03846906803356701\n"
     ]
    }
   ],
   "source": [
    "# use ridge classifier with built in cross validation\n",
    "rc = linear_model.RidgeClassifierCV(alphas=np.logspace(0.01, 3, 20))\n",
    "rc.fit(X_train, y_train)\n",
    "print(f\"Classification report:\\n\"\n",
    "    f\"{metrics.classification_report(y_test, predicted)}\\n\")\n",
    "\n",
    "cross_validate(X, y, rc, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'C': 10.011519555381687, 'penalty': 'l2', 'solver': 'newton-cg', 'tol': 5.721369546523265}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "           1       0.90      0.95      0.92        73\n",
      "           2       0.99      0.99      0.99        71\n",
      "           3       0.99      0.96      0.97        70\n",
      "           4       0.97      0.98      0.98        63\n",
      "           5       0.97      0.97      0.97        89\n",
      "           6       0.97      0.97      0.97        76\n",
      "           7       1.00      0.98      0.99        65\n",
      "           8       0.96      0.88      0.92        78\n",
      "           9       0.91      0.96      0.93        74\n",
      "\n",
      "    accuracy                           0.96       719\n",
      "   macro avg       0.96      0.96      0.96       719\n",
      "weighted avg       0.96      0.96      0.96       719\n",
      "\n",
      "Cross validation score, using 10 folds\n",
      "Test 0: 0.9055555555555556\n",
      "Test 1: 0.95\n",
      "Test 2: 0.8833333333333333\n",
      "Test 3: 0.9055555555555556\n",
      "Test 4: 0.9666666666666667\n",
      "Test 5: 0.9666666666666667\n",
      "Test 6: 0.9722222222222222\n",
      "Test 7: 0.9329608938547486\n",
      "Test 8: 0.8994413407821229\n",
      "Test 9: 0.9385474860335196\n",
      "Mean score: 0.932094972067039, std: 0.030358217055575495\n"
     ]
    }
   ],
   "source": [
    "# create logistic regression classifier\n",
    "params = {'solver': ['newton-cg', 'lbfgs', 'sag'], 'penalty': ['l2'], 'tol': np.logspace(0.01, 3, 5), 'C': np.logspace(0.001, 2, 5)},\n",
    "lr_clf = fit_grid_search_params(params, linear_model.LogisticRegression())\n",
    "\n",
    "cross_validate(X, y, lr_clf, cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'base_estimator': DecisionTreeClassifier(max_depth=4), 'n_estimators': 150}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        60\n",
      "           1       0.92      0.93      0.93        73\n",
      "           2       0.97      0.90      0.93        71\n",
      "           3       1.00      0.89      0.94        70\n",
      "           4       0.98      0.95      0.97        63\n",
      "           5       0.99      0.92      0.95        89\n",
      "           6       1.00      0.91      0.95        76\n",
      "           7       0.98      0.97      0.98        65\n",
      "           8       0.75      0.97      0.84        78\n",
      "           9       0.90      0.95      0.92        74\n",
      "\n",
      "    accuracy                           0.94       719\n",
      "   macro avg       0.95      0.94      0.94       719\n",
      "weighted avg       0.95      0.94      0.94       719\n",
      "\n",
      "Cross validation score, using 5 folds\n",
      "Test 0: 0.8777777777777778\n",
      "Test 1: 0.8138888888888889\n",
      "Test 2: 0.8997214484679665\n",
      "Test 3: 0.935933147632312\n",
      "Test 4: 0.8746518105849582\n",
      "Mean score: 0.8803946146703806, std: 0.03980174454471072\n"
     ]
    }
   ],
   "source": [
    "# create adaboost classifier using decision tree\n",
    "params = {'n_estimators': [75,100,125,150], 'base_estimator': [tree.DecisionTreeClassifier(max_depth=2), tree.DecisionTreeClassifier(max_depth=3), tree.DecisionTreeClassifier(max_depth=4)]}\n",
    "ad = fit_grid_search_params(params, ensemble.AdaBoostClassifier())\n",
    "\n",
    "cross_validate(X, y, ad, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'base_estimator': DecisionTreeClassifier(max_depth=4), 'n_estimators': 15, 'n_jobs': -1}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        60\n",
      "           1       0.38      0.38      0.38        73\n",
      "           2       0.53      0.90      0.67        71\n",
      "           3       0.77      0.80      0.78        70\n",
      "           4       0.62      0.92      0.74        63\n",
      "           5       0.89      0.65      0.75        89\n",
      "           6       0.83      0.89      0.86        76\n",
      "           7       0.76      0.82      0.79        65\n",
      "           8       0.33      0.03      0.05        78\n",
      "           9       0.83      0.81      0.82        74\n",
      "\n",
      "    accuracy                           0.70       719\n",
      "   macro avg       0.69      0.72      0.68       719\n",
      "weighted avg       0.69      0.70      0.67       719\n",
      "\n",
      "Cross validation score, using 5 folds\n",
      "Test 0: 0.7611111111111111\n",
      "Test 1: 0.6722222222222223\n",
      "Test 2: 0.8690807799442897\n",
      "Test 3: 0.8050139275766016\n",
      "Test 4: 0.7799442896935933\n",
      "Mean score: 0.7774744661095635, std: 0.06403441720035061\n"
     ]
    }
   ],
   "source": [
    "# bagging using decision tree\n",
    "params = {'n_estimators': [5, 10, 15, 20], \n",
    "          'base_estimator': [tree.DecisionTreeClassifier(max_depth=2), tree.DecisionTreeClassifier(max_depth=3), tree.DecisionTreeClassifier(max_depth=4)],\n",
    "          'n_jobs': [-1]}\n",
    "\n",
    "bag = fit_grid_search_params(params, ensemble.BaggingClassifier())\n",
    "\n",
    "cross_validate(X, y, bag, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal setting: {'criterion': 'entropy', 'max_depth': 5, 'n_estimators': 150, 'n_jobs': -1}\n",
      "\n",
      "Results on test set\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        60\n",
      "           1       0.87      0.92      0.89        73\n",
      "           2       0.98      0.90      0.94        71\n",
      "           3       0.93      0.93      0.93        70\n",
      "           4       0.97      0.95      0.96        63\n",
      "           5       0.95      0.94      0.95        89\n",
      "           6       0.97      0.97      0.97        76\n",
      "           7       0.90      0.98      0.94        65\n",
      "           8       0.96      0.86      0.91        78\n",
      "           9       0.89      0.92      0.91        74\n",
      "\n",
      "    accuracy                           0.94       719\n",
      "   macro avg       0.94      0.94      0.94       719\n",
      "weighted avg       0.94      0.94      0.94       719\n",
      "\n",
      "Cross validation score, using 5 folds\n",
      "Test 0: 0.9027777777777778\n",
      "Test 1: 0.8777777777777778\n",
      "Test 2: 0.9164345403899722\n",
      "Test 3: 0.935933147632312\n",
      "Test 4: 0.8774373259052924\n",
      "Mean score: 0.9020721138966264, std: 0.022585103093732923\n"
     ]
    }
   ],
   "source": [
    "# create random forest classfier\n",
    "\n",
    "params = {'n_estimators': [50,100,150,200], \n",
    "          'criterion': ['gini', 'entropy'], \n",
    "          'max_depth': [3,4,5], \n",
    "          'n_jobs': [-1]}\n",
    "\n",
    "rf = fit_grid_search_params(params, ensemble.RandomForestClassifier())\n",
    "\n",
    "cross_validate(X, y, rf, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
